{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c46938ec-ea16-4876-bed6-b9c4f9f81b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from torch.nn import functional\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "%matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "#### `svetlanna`\n",
    "\n",
    "from svetlanna.units import ureg\n",
    "\n",
    "from svetlanna import SimulationParameters\n",
    "from svetlanna.parameters import ConstrainedParameter\n",
    "\n",
    "from svetlanna import Wavefront\n",
    "from svetlanna.transforms import ToWavefront\n",
    "\n",
    "from svetlanna.elements import FreeSpace, Aperture, RectangularAperture, DiffractiveLayer\n",
    "from svetlanna.setup import LinearOpticalSetup\n",
    "from svetlanna.detector import Detector, DetectorProcessorClf\n",
    "\n",
    "from svetlanna.visualization import show_stepwise_forward\n",
    "\n",
    "from svetlanna.clerk import Clerk\n",
    "\n",
    "#### `src`\n",
    "\n",
    "# dataset of wavefronts\n",
    "from src.wf_datasets import DatasetOfWavefronts\n",
    "\n",
    "# training and evaluation loops\n",
    "from src.clf_loops import onn_train_clf, onn_validate_clf\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d750ff80-f5e3-4f34-bb26-9793b7074f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specified parameters:\n",
      "lambda = 749.481 um\n",
      "neuron size = 397.225 um\n",
      "Layer size (in neurons): 200 x 200 = 40000\n",
      "Layer size (in cm): 7.944500137 x 7.944500137\n"
     ]
    }
   ],
   "source": [
    "c_const = 299_792_458 # [m / s]\n",
    "\n",
    "working_frequency = 400 * ureg.GHz  # [Hz]\n",
    "working_wavelength = c_const / working_frequency  # [m]\n",
    "\n",
    "# neuron size (square)\n",
    "neuron_size = 0.53 * working_wavelength  # [m]\n",
    "\n",
    "print('Specified parameters:')\n",
    "# uncomment next two lines!\n",
    "print(f'lambda = {working_wavelength * 1e6:.3f} um')\n",
    "print(f'neuron size = {neuron_size * 1e6:.3f} um') # IR\n",
    "\n",
    "# an actual zone where weights will be updated during a training process\n",
    "ALL_SIZE = (200, 200)  # for example (100, 100) neurons\n",
    "USE_APERTURES = False\n",
    "\n",
    "a\n",
    "# number of neurons in simulation\n",
    "x_layer_nodes = ALL_SIZE[1]\n",
    "y_layer_nodes = ALL_SIZE[0]\n",
    "\n",
    "# calculate physical size of each layer in [m]\n",
    "x_layer_size_m = x_layer_nodes * neuron_size  # [m]\n",
    "y_layer_size_m = y_layer_nodes * neuron_size\n",
    "\n",
    "print(f'Layer size (in neurons): {x_layer_nodes} x {y_layer_nodes} = {x_layer_nodes * y_layer_nodes}')\n",
    "print(f'Layer size (in cm): {x_layer_size_m * 1e2} x {y_layer_size_m * 1e2}')\n",
    "\n",
    "\n",
    "\n",
    "# simulation parameters for the rest of the notebook!\n",
    "SIM_PARAMS = SimulationParameters(\n",
    "    axes={\n",
    "        \"W\" : torch.linspace(-x_layer_size_m / 2, x_layer_size_m / 2, x_layer_nodes),\n",
    "        \"H\" : torch.linspace(-y_layer_size_m / 2, y_layer_size_m / 2, y_layer_nodes),\n",
    "        \"wavelength\": working_wavelength\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08d76c6f-ffe3-4116-8d87-4f31e7ea6c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a directory for a dataset\n",
    "MNIST_DATA_FOLDER = './data'  # folder to store data\n",
    "\n",
    "if not os.path.exists(MNIST_DATA_FOLDER):\n",
    "    os.makedirs(MNIST_DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "876d404a-520f-4998-81c0-5dfba12c6b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN (images)\n",
    "mnist_train_ds = torchvision.datasets.MNIST(\n",
    "    root=MNIST_DATA_FOLDER,\n",
    "    train=True,  # for train dataset\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "# TEST (images)\n",
    "mnist_test_ds = torchvision.datasets.MNIST(\n",
    "    root=MNIST_DATA_FOLDER,\n",
    "    train=False,  # for test dataset\n",
    "    download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27c5f947-fa2b-478d-a29f-a1408f65f23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select modulation type\n",
    "MODULATION_TYPE = 'amp'  # using ONLY amplitude to encode each picture in a Wavefront!\n",
    "\n",
    "resize_y = int(DETECTOR_SIZE[0] / 2)\n",
    "resize_x = int(DETECTOR_SIZE[1] / 2)  # shape for transforms.Resize\n",
    "\n",
    "# paddings along OY\n",
    "pad_top = int((y_layer_nodes - resize_y) / 2)\n",
    "pad_bottom = y_layer_nodes - pad_top - resize_y\n",
    "# paddings along OX\n",
    "pad_left = int((x_layer_nodes - resize_x) / 2)\n",
    "pad_right = x_layer_nodes - pad_left - resize_x  # params for transforms.Pad\n",
    "\n",
    "# compose all transforms!\n",
    "image_transform_for_ds = transforms.Compose(\n",
    "  [\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Resize(\n",
    "          size=(resize_y, resize_x),\n",
    "          interpolation=InterpolationMode.NEAREST,\n",
    "      ),\n",
    "      transforms.Pad(\n",
    "          padding=(\n",
    "              pad_left,  # left padding\n",
    "              pad_top,  # top padding\n",
    "              pad_right,  # right padding\n",
    "              pad_bottom  # bottom padding\n",
    "          ),\n",
    "          fill=0,\n",
    "      ),  # padding to match sizes!\n",
    "      ToWavefront(modulation_type=MODULATION_TYPE)  # <- select modulation type!!!\n",
    "  ]  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a8ad96b-3164-48d1-8c41-d19e0c517826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 60000\n",
      "Test data : 10000\n"
     ]
    }
   ],
   "source": [
    "# TRAIN dataset of WAVEFRONTS\n",
    "mnist_wf_train_ds = DatasetOfWavefronts(\n",
    "    init_ds=mnist_train_ds,  # dataset of images\n",
    "    transformations=image_transform_for_ds,  # image transformation\n",
    "    sim_params=SIM_PARAMS,  # simulation parameters\n",
    ")\n",
    "\n",
    "# TEST dataset of WAVEFRONTS\n",
    "mnist_wf_test_ds = DatasetOfWavefronts(\n",
    "    init_ds=mnist_test_ds,  # dataset of images\n",
    "    transformations=image_transform_for_ds,  # image transformation\n",
    "    sim_params=SIM_PARAMS,  # simulation parameters\n",
    ")\n",
    "\n",
    "print(f'Train data: {len(mnist_train_ds)}')\n",
    "print(f'Test data : {len(mnist_test_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53db3394-3755-4918-9c9a-d50af3c77033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APAACzBVBJJwAO9dnp/wm8damu6Dw5dRjGf9IKw/+hkVPffCnWNJa7XVNV0Kxa1hErrNe/M2cnYqgElsAHpjkc1wlAODkV694W8c654t8M6n4TuvEctrrFw0cun3c0/lq+3AMJcDK5AyOeTkd+fPvGFn4gsvEtzF4m89tUG1ZJJjuMgUBVYN/EMKOe9YVXtK0bUtdvVs9LsZ7y4YgbIULYycZPoPc8V6lpfwh0/w7p66z8RdXj0y2z8llC4aWQ+mRn8lz9RXPfE3x1pvi46TYaPZTQadpMJghluWDSyrhQM9SMBe5Oc5NcBV7Tda1XRZJJNK1O8sXkG12tZ2iLD0JUjNQ3l9eahN517dT3MvTfNIXb16n6mq9Ff/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA90lEQVR4AWNgGMyAWUhIqK5jvdSy/9/rQe5kgTlWjs3KRiAYxHsyKfDzxYMgFiOIAALDvfwQBsO/pK8Mz97fhPLAlNDtvyBwbNv3j8jCUHbAnOy/f89yM2jPwiLJwMc4628UqgQTnPvp/0eGFAQXLg5lcO/764YuhuArf3y4IAfmfoQwlBX44e/fckkMYaiA7q6/f6dJ45IViP3zdzcuSQaGn39/OkBl4WEL4euFmLIwXDuETav6lKfAIPy1DYucRNFdUPCe9MOUE3e6CpI6FogZSEKrwbFyOIATQ5v5mkcgXV9auVGlwK4NDGRguL75b88HVDla8QBFF16ADQA8sQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train_ds[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "849494fd-0a2e-436b-a0ea-61838cdcc874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_examples(n_examples= 4, seed=78):\n",
    "    random.seed(seed)\n",
    "    train_examples_ids = random.sample(range(len(mnist_train_ds)), n_examples)\n",
    "    \n",
    "    all_examples_wavefronts = []\n",
    "    \n",
    "    n_lines = 3\n",
    "    fig, axs = plt.subplots(n_lines, n_examples, figsize=(n_examples * 3, n_lines * 3.2))\n",
    "    for ind_ex, ind_train in enumerate(train_examples_ids):\n",
    "        image, label = mnist_train_ds[ind_train]\n",
    "        \n",
    "        axs[0][ind_ex].set_title(f'id={ind_train} [{label}]')\n",
    "        axs[0][ind_ex].imshow(image, cmap='gray')\n",
    "    \n",
    "        wavefront, wf_label = mnist_wf_train_ds[ind_train]\n",
    "        assert isinstance(wavefront, Wavefront)\n",
    "    \n",
    "        all_examples_wavefronts.append(wavefront)\n",
    "    \n",
    "        axs[1][ind_ex].set_title(f'$|WF|^2$')\n",
    "        axs[1][ind_ex].imshow(wavefront.intensity, cmap='gray')\n",
    "        \n",
    "        axs[2][ind_ex].set_title(f'phase of $WF$')\n",
    "        axs[2][ind_ex].imshow(wavefront.phase, cmap='gray')\n",
    "#plot_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cd343b7-e1d0-43d8-b98b-9fa7e33b01cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between layers is 2.998 cm\n"
     ]
    }
   ],
   "source": [
    "NUM_OF_DIFF_LAYERS = 3  # number of diffractive layers\n",
    "FREE_SPACE_DISTANCE = 40 * working_wavelength  # [m] - distance between difractive layers\n",
    "\n",
    "print(f'Distance between layers is {FREE_SPACE_DISTANCE * 1e2:.3f} cm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6281b641-1de1-46fa-a4b1-2889247c6ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PHASE = 2 * np.pi\n",
    "\n",
    "FREESPACE_METHOD = 'AS'  # we use an angular spectrum method\n",
    "\n",
    "INIT_PHASES = torch.ones(NUM_OF_DIFF_LAYERS) * np.pi # todo: was ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "22411824-c546-4721-99cb-1564445a4c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAKE A LOOK! CODE HERE IS READY\n",
    "import svetlanna.parameters as params\n",
    "def get_const_phase_layer(\n",
    "    sim_params: SimulationParameters,\n",
    "    value: float, \n",
    "    max_phase=2 * torch.pi\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns DiffractiveLayer with a constant phase mask.\n",
    "    \"\"\"\n",
    "    x_nodes, y_nodes = sim_params.axes_size(axs=('W', 'H'))\n",
    "\n",
    "    const_mask = torch.ones(size=(y_nodes, x_nodes))* value\n",
    "    \n",
    "    return DiffractiveLayer(\n",
    "        simulation_parameters=sim_params,\n",
    "        mask=params.Parameter(const_mask) \n",
    "    )\n",
    "    #\n",
    "    #ConstrainedParameter(\n",
    "    #        const_mask,\n",
    "    #        min_value=0,\n",
    "    #        max_value=max_phase\n",
    "    #    ),  # HERE WE ARE USING CONSTRAINED PARAMETER! Phases are learnable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f4e3cf49-5e7f-4fe1-81c9-dd90021c9406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elements_list(\n",
    "    num_layers,\n",
    "    simulation_parameters,\n",
    "    freespace_method,\n",
    "    phase_values,\n",
    "    apertures=False,\n",
    "    aperture_size=(100, 100)\n",
    "):\n",
    "    free_space = lambda: FreeSpace(simulation_parameters, FREE_SPACE_DISTANCE, freespace_method) # Here happens unnecessary blurring. Or not?\n",
    "    aperture = lambda: [RectangularAperture(simulation_parameters, aperture_size)] if apertures else []\n",
    "    layer = lambda i: aperture() + [get_const_phase_layer(simulation_parameters, phase_values[i]), free_space()]\n",
    "    layers = list(itertools.chain(*map(layer, range(num_layers))))\n",
    "    return [free_space()] + layers + aperture() + [Detector(simulation_parameters,'intensity',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "092f0c4c-6b3a-4d43-815e-eefc747922b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FreeSpace(),\n",
       " DiffractiveLayer(\n",
       "   (mask_svtlnn_inner_parameter): InnerParameterStorageModule()\n",
       " ),\n",
       " FreeSpace(),\n",
       " DiffractiveLayer(\n",
       "   (mask_svtlnn_inner_parameter): InnerParameterStorageModule()\n",
       " ),\n",
       " FreeSpace(),\n",
       " DiffractiveLayer(\n",
       "   (mask_svtlnn_inner_parameter): InnerParameterStorageModule()\n",
       " ),\n",
       " FreeSpace(),\n",
       " Detector()]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "architecture_elements_list = get_elements_list(\n",
    "    num_layers=NUM_OF_DIFF_LAYERS,\n",
    "    simulation_parameters=SIM_PARAMS,\n",
    "    freespace_method=FREESPACE_METHOD,\n",
    "    phase_values=INIT_PHASES,\n",
    "    apertures=USE_APERTURES,\n",
    "    aperture_size=DETECTOR_SIZE\n",
    ")\n",
    "architecture_elements_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f2006774-6cd7-4c26-ac94-ea6ca05214b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in the system (including Detector): 8\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of elements in the system (including Detector): {len(architecture_elements_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "727a9d29-8b31-47e9-b0b0-fdae9d44c813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_setup(simulation_parameters, apertures=False):\n",
    "    \"\"\"\n",
    "    Returns an optical setup. Recreates all elements.\n",
    "    \"\"\"\n",
    "    elements_list = get_elements_list(\n",
    "        num_layers=NUM_OF_DIFF_LAYERS,\n",
    "        simulation_parameters=SIM_PARAMS,\n",
    "        freespace_method=FREESPACE_METHOD,\n",
    "        phase_values=INIT_PHASES,\n",
    "        apertures=apertures,\n",
    "        aperture_size=DETECTOR_SIZE\n",
    "    )  # recreate a list of elements\n",
    "\n",
    "    return LinearOpticalSetup(elements=elements_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "762206a1-1bc1-4725-b7dc-53dcb252d66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creaye an optical setup\n",
    "optical_setup = get_setup(SIM_PARAMS, apertures=USE_APERTURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "223bd522-f656-45a7-8f0e-ef74ed6aa9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check the architecture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "90ec1286-321e-4444-9f3c-e9d53c5e381a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): FreeSpace()\n",
       "  (1): DiffractiveLayer(\n",
       "    (mask_svtlnn_inner_parameter): InnerParameterStorageModule()\n",
       "  )\n",
       "  (2): FreeSpace()\n",
       "  (3): DiffractiveLayer(\n",
       "    (mask_svtlnn_inner_parameter): InnerParameterStorageModule()\n",
       "  )\n",
       "  (4): FreeSpace()\n",
       "  (5): DiffractiveLayer(\n",
       "    (mask_svtlnn_inner_parameter): InnerParameterStorageModule()\n",
       "  )\n",
       "  (6): FreeSpace()\n",
       "  (7): Detector()\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optical_setup.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9480488a-4bb6-4d81-8f68-fa09c4b73c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ind = np.random.randint(0, len(mnist_wf_train_ds))\n",
    "\n",
    "example_wf, example_label = mnist_wf_train_ds[random_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fee1d93e-a61e-4744-be92-f715dee19db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label of selected wavefront: 1\n"
     ]
    }
   ],
   "source": [
    "print(f'Label of selected wavefront: {example_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a7810b55-235b-49ea-9f48-337c87efeca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    show_stepwise_forward(\n",
    "    optical_setup,\n",
    "    input= example_wf,  # TODO: add a wavefront to propagate!\n",
    "    simulation_parameters=SIM_PARAMS,\n",
    "    types_to_plot=(\"I\", \"phase\")  # TODO: check different types! \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "91d1e9b5-8703-4201-8743-b54eac91da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e8c1e018-fb72-480e-b7a4-5eaf29145ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector_segment_size = 6.4 * working_wavelength  # in neurons (int)\n",
    "detector_segment_size_m = detector_segment_size // neuron_size  # in [m]\n",
    "detector_segment_size_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7c8aa135-9ddb-41c2-89d3-bf25d37aa04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detector_mask(\n",
    "    segment_size_in_neurons : int,\n",
    "    segment_dist : int,\n",
    "    order_of_digits=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "):\n",
    "    if not len(order_of_digits) == NUMBER_OF_CLASSES:\n",
    "        print('Wrong ordering list!')\n",
    "\n",
    "    t = torch.ones(ALL_SIZE) * -1\n",
    "    upper = [(-1, 1), (0, 1), (1, 1)]\n",
    "    lower = [(-1, -1), (0, -1), (1, -1)]\n",
    "    middle = [(-1.5, 0), (-0.5, 0), (0.5, 0), (1.5, 0)]\n",
    "    \n",
    "    for i, (x, y) in enumerate(lower + middle + upper):\n",
    "        if not i in order_of_digits:\n",
    "            print('Wrong ordering list!')\n",
    "        x_start = int(segment_size_in_neurons + x * segment_dist + ALL_SIZE[1] // 2 - segment_size_in_neurons // 2 - segment_dist // 2)\n",
    "        y_start = int(segment_size_in_neurons + y * segment_dist + ALL_SIZE[0] // 2 - segment_size_in_neurons // 2 - segment_dist // 2)\n",
    "        t[y_start: y_start + int(segment_size_in_neurons), x_start: x_start + int(segment_size_in_neurons)] = order_of_digits[i]\n",
    "        print(f'Adding segment for class {order_of_digits[i]} which is {i}th at ({x_start}, {y_start})')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "31171243-b32c-4261-b850-4c0a6cd5deb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding segment for class 0 which is 0th at (70, 70)\n",
      "Adding segment for class 1 which is 1th at (94, 70)\n",
      "Adding segment for class 2 which is 2th at (118, 70)\n",
      "Adding segment for class 3 which is 3th at (58, 94)\n",
      "Adding segment for class 4 which is 4th at (82, 94)\n",
      "Adding segment for class 5 which is 5th at (106, 94)\n",
      "Adding segment for class 6 which is 6th at (130, 94)\n",
      "Adding segment for class 7 which is 7th at (70, 118)\n",
      "Adding segment for class 8 which is 8th at (94, 118)\n",
      "Adding segment for class 9 which is 9th at (118, 118)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f61937e1510>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(get_detector_mask(detector_segment_size_m, detector_segment_size_m * 2), cmap='rainbow', vmin=-1, vmax=9)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c92ff6e7-d778-4aeb-a6b3-a1d828977f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZONES_ORDER = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  # TODO% specify the order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eeb7d86b-ab1d-4575-a757-a8ee00f350dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding segment for class 0 which is 0th at (70, 70)\n",
      "Adding segment for class 1 which is 1th at (94, 70)\n",
      "Adding segment for class 2 which is 2th at (118, 70)\n",
      "Adding segment for class 3 which is 3th at (58, 94)\n",
      "Adding segment for class 4 which is 4th at (82, 94)\n",
      "Adding segment for class 5 which is 5th at (106, 94)\n",
      "Adding segment for class 6 which is 6th at (130, 94)\n",
      "Adding segment for class 7 which is 7th at (70, 118)\n",
      "Adding segment for class 8 which is 8th at (94, 118)\n",
      "Adding segment for class 9 which is 9th at (118, 118)\n"
     ]
    }
   ],
   "source": [
    "DETECTOR_MASK = get_detector_mask(detector_segment_size_m, detector_segment_size_m * 2, ZONES_ORDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cb2adcfd-788b-4939-8471-b87b95c31b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DetectorProcessorOzcanClf object\n",
    "DETECTOR_PROCESSOR = DetectorProcessorClf(\n",
    "    simulation_parameters=SIM_PARAMS,\n",
    "    num_classes=NUMBER_OF_CLASSES,\n",
    "    segmented_detector=DETECTOR_MASK,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7b6bc396-cfbb-40a0-9bfe-e12987f9e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZONES_HIGHLIGHT_COLOR = 'w'\n",
    "ZONES_LW = 0.5\n",
    "selected_detector_mask = DETECTOR_PROCESSOR.segmented_detector.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bd154079-4d24-4787-9304-df43fd2b7d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zones_patches(detector_mask, delta = 0.5):\n",
    "    def zone_patch(ind_class):\n",
    "        idx_y, idx_x = (detector_mask == ind_class).nonzero(as_tuple=True)\n",
    "        return patches.Rectangle(\n",
    "            (idx_x[0] - delta, idx_y[0] - delta), \n",
    "            idx_x[-1] - idx_x[0] + 2 * delta, idx_y[-1] - idx_y[0] + 2 * delta, \n",
    "            linewidth=ZONES_LW, \n",
    "            edgecolor=ZONES_HIGHLIGHT_COLOR,\n",
    "            facecolor='none'\n",
    "        )\n",
    "    return [zone_patch(i) for i in range(NUMBER_OF_CLASSES)] # TODO: Zone order matters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5f2f5224-a035-4d45-bbf9-fb24f3473c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_bs = 20  # a batch size for training set\n",
    "val_bs = 8\n",
    "\n",
    "# TODO: specify a random seed\n",
    "train_val_split_seed = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1eaacee1-7361-4d4c-83d6-815ccebaebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist_wf_train_ds\n",
    "train_wf_ds, val_wf_ds = torch.utils.data.random_split(\n",
    "    dataset=mnist_wf_train_ds,\n",
    "    lengths=[55000, 5000],  # sizes from the article\n",
    "    generator=torch.Generator().manual_seed(train_val_split_seed)  # for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "43c3fbef-5a00-4784-a5c8-85c47231890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wf_loader = torch.utils.data.DataLoader(\n",
    "    train_wf_ds,\n",
    "    batch_size=train_bs,\n",
    "    shuffle=True,\n",
    "    # num_workers=2,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "val_wf_loader = torch.utils.data.DataLoader(\n",
    "    val_wf_ds,\n",
    "    batch_size=val_bs,\n",
    "    shuffle=False,\n",
    "    # num_workers=2,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b61be8c4-f379-4987-a314-2f7e17eef84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8fbf825c-e9ca-4f94-b1d6-da4921748cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adam_optimizer(net):\n",
    "    return torch.optim.Adam(\n",
    "        params=net.parameters(),  # NETWORK PARAMETERS!\n",
    "        lr=LR\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "390a425d-cff3-44ba-81f6-2fe467c52a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func_clf = nn.CrossEntropyLoss()\n",
    "loss_func_name = 'CE loss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6712d830-8a01-4f32-b2be-17631fad71f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are just importing them from src folder\n",
    "from src.clf_loops import onn_train_clf, onn_validate_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0523bf4f-a304-4240-aae7-f12ff846565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_wf_loader = torch.utils.data.DataLoader(\n",
    "    mnist_wf_test_ds,\n",
    "    batch_size=10,\n",
    "    shuffle=False,\n",
    "    # num_workers=2\n",
    "    drop_last=False,\n",
    ")  # data loader for a test MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "28abb09f-0423-47da-8b7f-c5e4a1a3aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    test_losses_0, _, test_accuracy_0 = onn_validate_clf(\n",
    "        optical_setup.net,  # optical network composed in 3.\n",
    "        test_wf_loader,  # dataloader of training set\n",
    "        DETECTOR_PROCESSOR,  # detector processor\n",
    "        loss_func_clf,\n",
    "        device=DEVICE,\n",
    "        show_process=True,\n",
    "    )  # evaluate the model\n",
    "    \n",
    "    print(\n",
    "        'Results before training on TEST set:\\n' + \n",
    "        f'\\t{loss_func_name} : {np.mean(test_losses_0):.6f}\\n' +\n",
    "        f'\\tAccuracy : {(test_accuracy_0 * 100):>0.1f} %'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f415de52-bad7-492d-8fa1-d57a96d30426",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3\n",
    "\n",
    "TORCH_SEED = 4\n",
    "\n",
    "today_date = datetime.today().strftime('%d-%m-%Y_%H-%M')  # date for a results folder name\n",
    "\n",
    "RESULTS_FOLDER = f'results/exp_{today_date}'\n",
    "RESULTS_FOLDER\n",
    "\n",
    "VARIABLES = {    \n",
    "    'wavelength': working_wavelength,  # working wavelength, in [m]\n",
    "    'neuron_size': neuron_size,  # size of a pixel for DiffractiveLayers, in [m]\n",
    "    'mesh_size': ALL_SIZE,  # full size of a layer = numerical mesh\n",
    "    'use_apertures': USE_APERTURES,  # if we need to add apertures before each DiffractieLayer\n",
    "    'aperture_size': DETECTOR_SIZE,  # size of each aperture = a detector square for classes zones\n",
    "    \n",
    "    # DETECTOR ZONES\n",
    "    'detector_segment_size': detector_segment_size_m,  # size of each square class zone on a detector, in [m]\n",
    "    'segments_order': ZONES_ORDER,\n",
    "\n",
    "    # RANDOM SEEDS\n",
    "    'train_val_seed': train_val_split_seed,\n",
    "    'torch_seed': TORCH_SEED,\n",
    "    \n",
    "    # NETWORK - SECTION 3 of the notebook\n",
    "    'free_space_distance': FREE_SPACE_DISTANCE,  # constant free space distance for a network, in [m]\n",
    "    \n",
    "    # OPTICAL NETWORK LEARNING - SECTION 4 of the notebook\n",
    "    'train_batch_size': train_bs,  # batch sizes for training\n",
    "    'val_batch_size': val_bs,\n",
    "    'adam_lr': LR,  # learning rate for Adam optimizer\n",
    "    'number_of_epochs': NUM_EPOCHS,  # number of epochs to train\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6c8b86ab-1e41-4e65-b1af-85b2c481e75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(RESULTS_FOLDER):\n",
    "    os.makedirs(RESULTS_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "879df5ac-6ddf-42b1-a06f-3edec06055db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# save experiment conditions (VARIABLES dictionary)\n",
    "with open(f'{RESULTS_FOLDER}/conditions.json', 'w', encoding ='utf8') as json_file:\n",
    "    json.dump(VARIABLES, json_file, ensure_ascii = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "82f28b00-c1ba-4d2a-bc7b-ce56c5be77da",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(DETECTOR_MASK, f'{RESULTS_FOLDER}/detector_mask.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2ce8ef8d-dd2a-4bb6-8b6d-ca3792dbc6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_each = 1  # print each n'th epoch info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6adb8840-df52-425c-bacf-0cfea6e8c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = None  # sheduler for a lr tuning during training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c58a2fc5-fed3-4730-ae75-bb814ab0daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate a system to restart training!\n",
    "optical_setup_to_train = get_setup(SIM_PARAMS, apertures=USE_APERTURES)\n",
    "# Link optimizer to a recreated net!\n",
    "optimizer_clf = get_adam_optimizer(optical_setup_to_train.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a0d3cd09-a569-40ef-ab30-a1b4a1ce6531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 of 3: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|████████████████████████████████| 2750/2750 [08:11<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results\n",
      "\tCE loss : 2.136797\n",
      "\tAccuracy : 49.2 %\n",
      "\t------------   491.88 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation: 100%|█████████████████████████████| 625/625 [00:24<00:00, 25.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results\n",
      "\tCE loss : 2.028201\n",
      "\tAccuracy : 69.9 %\n",
      "\t------------   24.87 s\n",
      "Epoch #2 of 3: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|████████████████████████████████| 2750/2750 [07:38<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results\n",
      "\tCE loss : 1.982592\n",
      "\tAccuracy : 75.5 %\n",
      "\t------------   458.58 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation: 100%|█████████████████████████████| 625/625 [00:23<00:00, 26.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results\n",
      "\tCE loss : 1.945290\n",
      "\tAccuracy : 79.3 %\n",
      "\t------------   23.92 s\n",
      "Epoch #3 of 3: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|████████████████████████████████| 2750/2750 [07:43<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results\n",
      "\tCE loss : 1.924811\n",
      "\tAccuracy : 80.4 %\n",
      "\t------------   463.08 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation: 100%|█████████████████████████████| 625/625 [00:28<00:00, 21.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results\n",
      "\tCE loss : 1.908081\n",
      "\tAccuracy : 81.3 %\n",
      "\t------------   28.75 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_epochs_losses = []\n",
    "val_epochs_losses = []  # to store losses of each epoch\n",
    "\n",
    "train_epochs_acc = []\n",
    "val_epochs_acc = []  # to store accuracies\n",
    "\n",
    "torch.manual_seed(TORCH_SEED)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    if (epoch == 0) or ((epoch + 1) % print_each == 0) or (epoch == n_epochs - 1):\n",
    "        print(f'Epoch #{epoch + 1} of {NUM_EPOCHS}: ', end='')\n",
    "        show_progress = True\n",
    "    else:\n",
    "        show_progress = False\n",
    "\n",
    "    # TRAIN\n",
    "    start_train_time = time.time()  # start time of the epoch (train)\n",
    "    train_losses, _, train_accuracy = onn_train_clf(\n",
    "        optical_setup_to_train.net,  # optical network composed in 3.\n",
    "        train_wf_loader,  # dataloader of training set\n",
    "        DETECTOR_PROCESSOR,  # detector processor\n",
    "        loss_func_clf,\n",
    "        optimizer_clf,\n",
    "        device=DEVICE,\n",
    "        show_process=show_progress,\n",
    "    )  # train the model\n",
    "    mean_train_loss = np.mean(train_losses)\n",
    "    \n",
    "    if (epoch == 0) or ((epoch + 1) % print_each == 0) or (epoch == n_epochs - 1):  # train info\n",
    "        print('Training results')\n",
    "        print(f'\\t{loss_func_name} : {mean_train_loss:.6f}')\n",
    "        print(f'\\tAccuracy : {(train_accuracy*100):>0.1f} %')\n",
    "        print(f'\\t------------   {time.time() - start_train_time:.2f} s')\n",
    "\n",
    "    # VALIDATION\n",
    "    start_val_time = time.time()  # start time of the epoch (validation)\n",
    "    val_losses, _, val_accuracy = onn_validate_clf(\n",
    "        optical_setup_to_train.net,  # optical network composed in 3.\n",
    "        val_wf_loader,  # dataloader of validation set\n",
    "        DETECTOR_PROCESSOR,  # detector processor\n",
    "        loss_func_clf,\n",
    "        device=DEVICE,\n",
    "        show_process=show_progress,\n",
    "    )  # evaluate the model\n",
    "    mean_val_loss = np.mean(val_losses)\n",
    "    \n",
    "    if (epoch == 0) or ((epoch + 1) % print_each == 0) or (epoch == n_epochs - 1):  # validation info\n",
    "        print('Validation results')\n",
    "        print(f'\\t{loss_func_name} : {mean_val_loss:.6f}')\n",
    "        print(f'\\tAccuracy : {(val_accuracy*100):>0.1f} %')\n",
    "        print(f'\\t------------   {time.time() - start_val_time:.2f} s')\n",
    "            \n",
    "    if scheduler:\n",
    "        scheduler.step(mean_val_loss) \n",
    "    \n",
    "    # save losses\n",
    "    train_epochs_losses.append(mean_train_loss)\n",
    "    val_epochs_losses.append(mean_val_loss)\n",
    "    # seve accuracies\n",
    "    train_epochs_acc.append(train_accuracy)\n",
    "    val_epochs_acc.append(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ad5984eb-710c-42c9-b717-bee14e51b893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAM+2pi filter: 532  24 488 23 438 22(s)\n",
    "# validation losses: 0.7656 0.8194 0.8294\n",
    "\n",
    "# SGD+2pi filter: the same!\n",
    "# Losses: Terrible! 14-15 percents.\n",
    "\n",
    "# ADAM without constrained parameters:\n",
    "# Losses:??? 70 79 81.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "964329df-2cd0-4292-b5b3-e3fbd15decba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# array with all losses\n",
    "\n",
    "all_lasses_header = ','.join([\n",
    "    f'{loss_func_name.split()[0]}_train', f'{loss_func_name.split()[0]}_val',\n",
    "    'accuracy_train', 'accuracy_val'\n",
    "])\n",
    "all_losses_array = np.array(\n",
    "    [train_epochs_losses, val_epochs_losses, train_epochs_acc, val_epochs_acc]\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0894e6ea-cdcb-473f-9892-ee87770c6941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath to save losses\n",
    "losses_filepath = f'{RESULTS_FOLDER}/training_curves.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "36b11433-0efc-4ae7-9053-dac28cee6967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving losses\n",
    "np.savetxt(\n",
    "    losses_filepath, all_losses_array,\n",
    "    delimiter=',', header=all_lasses_header, comments=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753a15ac-537f-453d-b404-31fbc5389ddb",
   "metadata": {},
   "source": [
    "#### Save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3535ab45-1f1f-418f-994c-c2c3c8f1a209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath to save the model\n",
    "model_filepath = f'{RESULTS_FOLDER}/optical_net.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "663cd0ce-07a1-4221-a09c-dadc7a34232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model\n",
    "torch.save(optical_setup_to_train.net.state_dict(), model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc99c34a-f60a-46ee-8d24-b43f28b7c4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490c48e8-7c6b-4117-99f3-50092076a6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
